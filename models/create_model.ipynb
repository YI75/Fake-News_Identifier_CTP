{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "51c06b0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\yusse\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\yusse\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\yusse\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6335, 4)\n",
      "Unnamed: 0    0\n",
      "title         0\n",
      "text          0\n",
      "label         0\n",
      "dtype: int64\n",
      "0\n",
      "      Unnamed: 0                                              title  \\\n",
      "0           8476                       You Can Smell Hillary’s Fear   \n",
      "1          10294  Watch The Exact Moment Paul Ryan Committed Pol...   \n",
      "2           3608        Kerry to go to Paris in gesture of sympathy   \n",
      "3          10142  Bernie supporters on Twitter erupt in anger ag...   \n",
      "4            875   The Battle of New York: Why This Primary Matters   \n",
      "...          ...                                                ...   \n",
      "6330        4490  State Department says it can't find emails fro...   \n",
      "6331        8062  The ‘P’ in PBS Should Stand for ‘Plutocratic’ ...   \n",
      "6332        8622  Anti-Trump Protesters Are Tools of the Oligarc...   \n",
      "6333        4021  In Ethiopia, Obama seeks progress on peace, se...   \n",
      "6334        4330  Jeb Bush Is Suddenly Attacking Trump. Here's W...   \n",
      "\n",
      "                                                   text label  \n",
      "0     Daniel Greenfield, a Shillman Journalism Fello...  FAKE  \n",
      "1     Google Pinterest Digg Linkedin Reddit Stumbleu...  FAKE  \n",
      "2     U.S. Secretary of State John F. Kerry said Mon...  REAL  \n",
      "3     — Kaydee King (@KaydeeKing) November 9, 2016 T...  FAKE  \n",
      "4     It's primary day in New York and front-runners...  REAL  \n",
      "...                                                 ...   ...  \n",
      "6330  The State Department told the Republican Natio...  REAL  \n",
      "6331  The ‘P’ in PBS Should Stand for ‘Plutocratic’ ...  FAKE  \n",
      "6332   Anti-Trump Protesters Are Tools of the Oligar...  FAKE  \n",
      "6333  ADDIS ABABA, Ethiopia —President Obama convene...  REAL  \n",
      "6334  Jeb Bush Is Suddenly Attacking Trump. Here's W...  REAL  \n",
      "\n",
      "[6335 rows x 4 columns]\n",
      "ORIGINAL TEXT: 0       daniel greenfield shillman journalism fellow f...\n",
      "1       google pinterest digg linkedin reddit stumbleu...\n",
      "2       us secretary state john f kerry said monday st...\n",
      "3       kaydee king kaydeeking november 9 2016 lesson ...\n",
      "4       primary day new york frontrunners hillary clin...\n",
      "                              ...                        \n",
      "6330    state department told republican national comm...\n",
      "6331    p pbs stand plutocratic pentagon posted oct 27...\n",
      "6332    antitrump protesters tools oligarchy reform al...\n",
      "6333    addis ababa ethiopia president obama convened ...\n",
      "6334    jeb bush suddenly attacking trump heres matter...\n",
      "Name: article_clean, Length: 6335, dtype: object\n",
      "CLEANDED TEXT: 0       daniel greenfield shillman journalism fellow f...\n",
      "1       google pinterest digg linkedin reddit stumbleu...\n",
      "2       us secretary state john f kerry said monday st...\n",
      "3       kaydee king kaydeeking november 9 2016 lesson ...\n",
      "4       primary day new york frontrunners hillary clin...\n",
      "                              ...                        \n",
      "6330    state department told republican national comm...\n",
      "6331    p pbs stand plutocratic pentagon posted oct 27...\n",
      "6332    antitrump protesters tools oligarchy reform al...\n",
      "6333    addis ababa ethiopia president obama convened ...\n",
      "6334    jeb bush suddenly attacking trump heres matter...\n",
      "Name: article_clean, Length: 6335, dtype: object\n",
      "(5068, 79241) <class 'numpy.ndarray'>\n",
      "79241 79241\n"
     ]
    }
   ],
   "source": [
    "# Import pandas for data handling\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "# NLTK is our Natural-Language-Took-Kit\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "# Libraries for helping us with strings\n",
    "import string\n",
    "# Regular Expression Library\n",
    "import re\n",
    "\n",
    "# Import our text vectorizers\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "# Import our classifiers\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "# Import some ML helper function\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "\n",
    "\n",
    "# Import our metrics to evaluate our model\n",
    "from sklearn import metrics\n",
    "\n",
    "\n",
    "# Library for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# You may need to download these from nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "stopwords = stopwords.words('english')\n",
    "\n",
    "# Import our data\n",
    "df = pd.read_csv('../data/fake_or_real_news.csv')\n",
    "print(df.shape)\n",
    "df.head(20)\n",
    "\n",
    "#no null values\n",
    "print(df.isnull().sum())\n",
    "#no dublicates\n",
    "print(df.duplicated().sum())\n",
    "\n",
    "df['label'].value_counts(normalize=True)\n",
    "\n",
    "#removing unwanted characters\n",
    "\n",
    "def remove_unwanted_char(a_string):    \n",
    "    a_string = re.sub(r\"(@\\[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)|^rt|http.+?\", \"\", str(a_string))\n",
    "    return a_string\n",
    "\n",
    "remove_unwanted_char(df['title'])\n",
    "print(df)\n",
    "\n",
    "# Remove all punctuation\n",
    "\n",
    "def remove_punctuation(a_string):    \n",
    "    a_string = re.sub(r'[^\\w\\s]','', str(a_string))\n",
    "    return a_string\n",
    "\n",
    "remove_punctuation(df['title'])\n",
    "\n",
    "def make_lower(a_string):\n",
    "    return a_string.lower()\n",
    "\n",
    "# Remove all stopwords\n",
    "\n",
    "def remove_stopwords(a_string):\n",
    "    # Break the sentence down into a list of words\n",
    "    words = word_tokenize(a_string)\n",
    "    \n",
    "    # Make a list to append valid words into\n",
    "    valid_words = []\n",
    "    \n",
    "    # Loop through all the words\n",
    "    for word in words:\n",
    "        \n",
    "        # Check if word is not in stopwords\n",
    "        if word not in stopwords:\n",
    "            \n",
    "            # If word not in stopwords, append to our valid_words\n",
    "            valid_words.append(word)\n",
    "\n",
    "    # Join the list of words together into a string\n",
    "    a_string = ' '.join(valid_words)\n",
    "\n",
    "    return a_string\n",
    "            \n",
    "remove_stopwords(str(df['title']))\n",
    "\n",
    "def text_pipeline(input_string):\n",
    "    input_string = make_lower(input_string)\n",
    "    input_string = remove_punctuation(input_string)\n",
    "    #input_string = lem_with_pos_tag(input_string)\n",
    "    input_string = remove_stopwords(input_string)    \n",
    "    return input_string\n",
    "\n",
    "\n",
    "df['article_clean'] = df['text']\n",
    "df['article_clean'] = df['text'].apply(text_pipeline)\n",
    "\n",
    "print(\"ORIGINAL TEXT:\", df['article_clean'])\n",
    "print(\"CLEANDED TEXT:\", df['article_clean'])\n",
    "\n",
    "\n",
    "# Build the Vectorizer\n",
    "\n",
    "# Define our `X` and `y` data. \n",
    "\n",
    "X = df['article_clean'].values\n",
    "\n",
    "y = df['label'].values\n",
    "\n",
    "# Split our data into testing and training like always. \n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Save the raw text for later just incase\n",
    "X_train_text = X_train\n",
    "X_test_text = X_test\n",
    "\n",
    "# Initialize our vectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# This makes your vocab matrix\n",
    "vectorizer.fit(X_train)\n",
    "\n",
    "pickle.dump(vectorizer, open('vectorizer.pkl', 'wb'))\n",
    "\n",
    "# This transforms your documents into vectors.\n",
    "X_train = vectorizer.transform(X_train)\n",
    "X_test = vectorizer.transform(X_test)\n",
    "\n",
    "print(X_train.shape, type(X))\n",
    "\n",
    "title_clean = vectorizer.get_feature_names()\n",
    "label = vectorizer.idf_\n",
    "\n",
    "print(len(title_clean), len(label))\n",
    "\n",
    "df_idf = pd.DataFrame.from_dict( {'article_clean': title_clean, 'label': label})\n",
    "\n",
    "df_idf = df_idf.sort_values(by='label', ascending=False)\n",
    "\n",
    "#Yussef \n",
    "#Initialize and train the model. Here we are using the MultinomialNB model.\n",
    "\n",
    "model = MultinomialNB(alpha=.05)\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "pickle.dump(model, open('model.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0777ec67",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "d422f2948c5dad1d9af385b0dc222fbae095116b06bf3332fe270b080b557eb3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
